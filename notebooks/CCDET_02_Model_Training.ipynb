{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb9e454-f66c-4c38-9781-bf3d9a7cc72d",
   "metadata": {},
   "source": [
    "# ğŸ“Œ **Credit Card Fraud Detection - Model Training & Evaluation**  \n",
    "\n",
    "Fraud detection is a **complex classification task** due to the extreme class imbalance.  \n",
    "This notebook focuses on **training and evaluating a deep learning model** to detect fraudulent transactions while minimizing false positives.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ **Workflow Overview**  \n",
    "\n",
    "### 1ï¸. Loading Preprocessed Data  \n",
    "- Load datasets (`X_train`, `X_val`, `X_test`, `y_train`, `y_val`, `y_test`) from `.npy` files.  \n",
    "- Verify the dataset structure before training.\n",
    "\n",
    "### 2ï¸. Defining Focal Loss for Handling Class Imbalance  \n",
    "- Fraud detection datasets are highly **imbalanced**, making standard **binary cross-entropy loss ineffective**.  \n",
    "- **Focal Loss** dynamically adjusts the importance of misclassified fraud cases.  \n",
    "\n",
    "### 3ï¸. Building the Neural Network Model  \n",
    "- Construct a deep learning model using:\n",
    "  - **Fully connected layers**\n",
    "  - **Batch normalization**\n",
    "  - **Dropout** for regularization.\n",
    "- Implement **focal loss** to improve fraud detection.\n",
    "\n",
    "### 4ï¸. Compiling & Summarizing the Model  \n",
    "- Define:\n",
    "  - Optimizer: `Adam`\n",
    "  - Loss function: `Focal Loss` or `Binary Crossentropy`\n",
    "  - Metrics: **Precision**, **Recall**, **AUC**  \n",
    "- Print the **model summary**.\n",
    "\n",
    "### 5ï¸. Training the Model  \n",
    "- Train the model using **early stopping** to prevent overfitting.  \n",
    "- Monitor `val_loss`, **precision**, and **recall**.\n",
    "\n",
    "### 6ï¸. Saving the Trained Model  \n",
    "- Save the trained model to the **models/** directory for future use.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02eb462-60ab-4f61-b941-488b882daed2",
   "metadata": {},
   "source": [
    "## **1. Load Required Libraries**  \n",
    "\n",
    "we need to import essential libraries for data processing, visualization, and model evaluation.\n",
    "\n",
    "- `numpy` and `pandas` for efficient data manipulation.\n",
    "- `seaborn` and `matplotlib` for exploratory data analysis (EDA).\n",
    "- `scikit-learn` for data preprocessing, model evaluation, and performance metrics.\n",
    "- `tensorflow` and `keras` for deep learning implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29f8c8-499b-4a81-ba8d-9eaff4f14649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Œ Step 1: Load Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99b4ce-a340-44a2-916c-a49b2c35ec6b",
   "metadata": {},
   "source": [
    "## **2. Load Preprocessed Data**  \n",
    "\n",
    "Before training, we **load the preprocessed dataset** saved in the previous notebook.  \n",
    "The dataset includes:  \n",
    "- **Scaled features** to ensure numerical consistency.  \n",
    "- **Balanced class labels** to mitigate the impact of class imbalance and improve fraud detection performance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "558d0816-733f-437e-b860-500b703542e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape: (159491, 30)\n",
      "X_val Shape: (39873, 30)\n",
      "X_test Shape: (85443, 30)\n",
      "y_train Shape: (159491,)\n",
      "y_val Shape: (39873,)\n",
      "y_test Shape: (85443,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# âœ… Define data paths\n",
    "data_path = \"/Users/adityaiyer/Desktop/Credit-Card-Fraud-Detection/data/processed/\"\n",
    "\n",
    "# âœ… Load preprocessed data\n",
    "X_train = np.load(data_path + \"X_train.npy\")\n",
    "X_val = np.load(data_path + \"X_val.npy\")\n",
    "X_test = np.load(data_path + \"X_test.npy\")\n",
    "y_train = np.load(data_path + \"y_train.npy\")\n",
    "y_val = np.load(data_path + \"y_val.npy\")\n",
    "y_test = np.load(data_path + \"y_test.npy\")\n",
    "\n",
    "# âœ… Verify shapes\n",
    "print(\"X_train Shape:\", X_train.shape)\n",
    "print(\"X_val Shape:\", X_val.shape)\n",
    "print(\"X_test Shape:\", X_test.shape)\n",
    "print(\"y_train Shape:\", y_train.shape)\n",
    "print(\"y_val Shape:\", y_val.shape)\n",
    "print(\"y_test Shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc1acc7-1e91-4ca4-9716-a8725181d73a",
   "metadata": {},
   "source": [
    "## **3. Implementing Focal Loss**  \n",
    "\n",
    "Since **fraud cases are rare**, using standard **Binary Cross-Entropy loss** may cause the model to favor the majority class (non-fraudulent transactions).  \n",
    "\n",
    "To address this, we implement **Focal Loss**, which dynamically adjusts the loss function to focus more on **misclassified fraud cases**, improving fraud detection performance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df8abfbc-9e73-403c-99a6-ab525984b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Focal loss for binary classification.\n",
    "    gamma > 1 penalizes easy examples\n",
    "    alpha balances weighting of classes (0 < alpha < 1)\n",
    "    \"\"\"\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        epsilon = 1e-7\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        cross_entropy = -y_true * K.log(y_pred) - (1.0 - y_true) * K.log(1.0 - y_pred)\n",
    "        weight = alpha * y_true * tf.pow((1.0 - y_pred), gamma) + \\\n",
    "                 (1.0 - alpha) * (1.0 - y_true) * tf.pow(y_pred, gamma)\n",
    "        \n",
    "        loss = weight * cross_entropy\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    return focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b45ff7-8f99-46a3-8aec-4f6191805640",
   "metadata": {},
   "source": [
    "## **4. Building and Compiling the Deep Learning Model**  \n",
    "\n",
    "We build a **fully connected deep neural network** using the **Keras Sequential API**.  \n",
    "The architecture consists of:  \n",
    "\n",
    "- **Input Layer:** Accepts the preprocessed transaction data (`X_train`).  \n",
    "- **Hidden Layers:**  \n",
    "  - **Dense Layers** with **ReLU activation** to extract transaction patterns.  \n",
    "  - **Batch Normalization** to stabilize learning and improve convergence.  \n",
    "  - **Dropout Layers** (30% dropout) to prevent overfitting.  \n",
    "- **Output Layer:**  \n",
    "  - A **single neuron** with a **sigmoid activation function** for binary classification (Fraud/Not Fraud).  \n",
    "\n",
    "This architecture ensures robust fraud detection while minimizing false positives.  \n",
    "\n",
    "## 4.1. Compile the Model \n",
    "\n",
    "The model is compiled with one of the following loss functions:  \n",
    "- **Focal Loss:** Adjusts the loss dynamically to focus more on misclassified fraud cases.  \n",
    "- **Binary Crossentropy:** A standard loss function for binary classification.  \n",
    "\n",
    "Additionally, we track the following performance metrics:  \n",
    "- **Precision:** Measures how many predicted fraud cases are actually fraud.  \n",
    "- **Recall:** Measures how many actual fraud cases are correctly identified.  \n",
    "- **AUC (Area Under the Curve):** Evaluates the model's ability to distinguish between fraud and non-fraud transactions.  \n",
    "\n",
    "These metrics provide a **balanced evaluation** of the modelâ€™s fraud detection capability.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "713a49c2-ae1e-424a-8337-72deda7964d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,936</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_6           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_7           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_8           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚         \u001b[38;5;34m7,936\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_6           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚         \u001b[38;5;34m1,024\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚        \u001b[38;5;34m32,896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_7           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚           \u001b[38;5;34m512\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_10 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_8           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_11 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m65\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,945</span> (199.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,945\u001b[0m (199.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,049</span> (195.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,049\u001b[0m (195.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# âœ… Define Model\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(X_train.shape[1],)),  # Correct way to define input layer\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
    "])\n",
    "\n",
    "# âœ… Choose Loss Function\n",
    "use_focal_loss = True\n",
    "\n",
    "if use_focal_loss:\n",
    "    loss_fn = focal_loss(gamma=2.0, alpha=0.25)\n",
    "else:\n",
    "    loss_fn = 'binary_crossentropy'\n",
    "\n",
    "# âœ… Compile Model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=loss_fn,\n",
    "    metrics=[\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')  # Adding AUC for better evaluation\n",
    "    ]\n",
    ")\n",
    "\n",
    "# âœ… Print Model Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcacb18-fefc-4830-9612-9f785cd5533d",
   "metadata": {},
   "source": [
    "## **6. Train the Model**  \n",
    "\n",
    "To optimize performance and prevent overfitting, we use **Early Stopping**, which:  \n",
    "- Monitors the **validation loss (`val_loss`)** during training.  \n",
    "- Stops training if the validation loss does not improve for a set number of epochs (**patience=5**).  \n",
    "- Restores the best model weights to ensure optimal performance.  \n",
    "\n",
    "The model is trained for **up to 50 epochs** with a **batch size of 512**, balancing efficiency and learning stability.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "261245e4-ea6a-475c-8b5a-f7e9d95ac4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - auc: 0.5966 - loss: 0.2713 - precision: 0.0022 - recall: 0.6029 - val_auc: 0.8322 - val_loss: 0.1245 - val_precision: 0.0033 - val_recall: 0.8116\n",
      "Epoch 2/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.6301 - loss: 0.1384 - precision: 0.0028 - recall: 0.6033 - val_auc: 0.8302 - val_loss: 0.0784 - val_precision: 0.0405 - val_recall: 0.7971\n",
      "Epoch 3/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - auc: 0.6277 - loss: 0.0876 - precision: 0.0056 - recall: 0.5437 - val_auc: 0.8330 - val_loss: 0.0527 - val_precision: 0.6707 - val_recall: 0.7971\n",
      "Epoch 4/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.5058 - loss: 0.0584 - precision: 0.0093 - recall: 0.3680 - val_auc: 0.8403 - val_loss: 0.0338 - val_precision: 0.8594 - val_recall: 0.7971\n",
      "Epoch 5/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.4915 - loss: 0.0394 - precision: 0.0317 - recall: 0.4035 - val_auc: 0.8394 - val_loss: 0.0226 - val_precision: 0.8833 - val_recall: 0.7681\n",
      "Epoch 6/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - auc: 0.5362 - loss: 0.0271 - precision: 0.0577 - recall: 0.3922 - val_auc: 0.8412 - val_loss: 0.0145 - val_precision: 0.8852 - val_recall: 0.7826\n",
      "Epoch 7/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - auc: 0.5057 - loss: 0.0187 - precision: 0.1015 - recall: 0.3562 - val_auc: 0.8464 - val_loss: 0.0100 - val_precision: 0.8594 - val_recall: 0.7971\n",
      "Epoch 8/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - auc: 0.5881 - loss: 0.0141 - precision: 0.1921 - recall: 0.4301 - val_auc: 0.8445 - val_loss: 0.0076 - val_precision: 0.8710 - val_recall: 0.7826\n",
      "Epoch 9/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.5678 - loss: 0.0105 - precision: 0.2742 - recall: 0.3739 - val_auc: 0.8474 - val_loss: 0.0056 - val_precision: 0.8710 - val_recall: 0.7826\n",
      "Epoch 10/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - auc: 0.5391 - loss: 0.0078 - precision: 0.2946 - recall: 0.3923 - val_auc: 0.8561 - val_loss: 0.0044 - val_precision: 0.8730 - val_recall: 0.7971\n",
      "Epoch 11/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - auc: 0.6126 - loss: 0.0060 - precision: 0.4037 - recall: 0.4067 - val_auc: 0.8657 - val_loss: 0.0038 - val_precision: 0.8594 - val_recall: 0.7971\n",
      "Epoch 12/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - auc: 0.6137 - loss: 0.0054 - precision: 0.4294 - recall: 0.4285 - val_auc: 0.8653 - val_loss: 0.0034 - val_precision: 0.8594 - val_recall: 0.7971\n",
      "Epoch 13/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.6843 - loss: 0.0042 - precision: 0.5541 - recall: 0.4935 - val_auc: 0.8771 - val_loss: 0.0028 - val_precision: 0.8594 - val_recall: 0.7971\n",
      "Epoch 14/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - auc: 0.7245 - loss: 0.0038 - precision: 0.5779 - recall: 0.4887 - val_auc: 0.8794 - val_loss: 0.0024 - val_precision: 0.8594 - val_recall: 0.7971\n",
      "Epoch 15/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - auc: 0.7290 - loss: 0.0029 - precision: 0.6298 - recall: 0.4952 - val_auc: 0.8886 - val_loss: 0.0024 - val_precision: 0.8594 - val_recall: 0.7971\n",
      "Epoch 16/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.7553 - loss: 0.0025 - precision: 0.6662 - recall: 0.5615 - val_auc: 0.8958 - val_loss: 0.0020 - val_precision: 0.8710 - val_recall: 0.7826\n",
      "Epoch 17/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.7349 - loss: 0.0021 - precision: 0.6826 - recall: 0.5093 - val_auc: 0.8927 - val_loss: 0.0019 - val_precision: 0.8689 - val_recall: 0.7681\n",
      "Epoch 18/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - auc: 0.7514 - loss: 0.0020 - precision: 0.6901 - recall: 0.4526 - val_auc: 0.9078 - val_loss: 0.0017 - val_precision: 0.8727 - val_recall: 0.6957\n",
      "Epoch 19/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.7635 - loss: 0.0018 - precision: 0.7275 - recall: 0.4577 - val_auc: 0.9098 - val_loss: 0.0016 - val_precision: 0.8491 - val_recall: 0.6522\n",
      "Epoch 20/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.8341 - loss: 0.0016 - precision: 0.7409 - recall: 0.4705 - val_auc: 0.9130 - val_loss: 0.0014 - val_precision: 0.8571 - val_recall: 0.6087\n",
      "Epoch 21/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - auc: 0.8235 - loss: 0.0014 - precision: 0.7373 - recall: 0.4942 - val_auc: 0.9130 - val_loss: 0.0011 - val_precision: 0.8478 - val_recall: 0.5652\n",
      "Epoch 22/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.8048 - loss: 0.0015 - precision: 0.7770 - recall: 0.4059 - val_auc: 0.9150 - val_loss: 0.0011 - val_precision: 0.8542 - val_recall: 0.5942\n",
      "Epoch 23/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.8493 - loss: 0.0012 - precision: 0.7794 - recall: 0.4537 - val_auc: 0.9190 - val_loss: 9.7780e-04 - val_precision: 0.8478 - val_recall: 0.5652\n",
      "Epoch 24/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.8758 - loss: 0.0010 - precision: 0.8279 - recall: 0.4679 - val_auc: 0.9160 - val_loss: 8.5300e-04 - val_precision: 0.8293 - val_recall: 0.4928\n",
      "Epoch 25/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.8441 - loss: 0.0012 - precision: 0.8161 - recall: 0.3834 - val_auc: 0.9189 - val_loss: 7.7810e-04 - val_precision: 0.8333 - val_recall: 0.5072\n",
      "Epoch 26/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - auc: 0.9040 - loss: 8.2458e-04 - precision: 0.8061 - recall: 0.4481 - val_auc: 0.9349 - val_loss: 7.7286e-04 - val_precision: 0.8372 - val_recall: 0.5217\n",
      "Epoch 27/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.9023 - loss: 9.0569e-04 - precision: 0.7990 - recall: 0.3984 - val_auc: 0.9371 - val_loss: 7.1710e-04 - val_precision: 0.8372 - val_recall: 0.5217\n",
      "Epoch 28/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - auc: 0.8769 - loss: 8.5489e-04 - precision: 0.8338 - recall: 0.4897 - val_auc: 0.9386 - val_loss: 6.7840e-04 - val_precision: 0.8372 - val_recall: 0.5217\n",
      "Epoch 29/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.9443 - loss: 7.3953e-04 - precision: 0.8758 - recall: 0.4808 - val_auc: 0.9471 - val_loss: 6.2110e-04 - val_precision: 0.8108 - val_recall: 0.4348\n",
      "Epoch 30/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - auc: 0.9310 - loss: 7.0417e-04 - precision: 0.8475 - recall: 0.3988 - val_auc: 0.9365 - val_loss: 6.6602e-04 - val_precision: 0.8333 - val_recall: 0.5072\n",
      "Epoch 31/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.9476 - loss: 6.2708e-04 - precision: 0.8635 - recall: 0.5033 - val_auc: 0.9418 - val_loss: 5.9546e-04 - val_precision: 0.8293 - val_recall: 0.4928\n",
      "Epoch 32/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.9092 - loss: 8.0269e-04 - precision: 0.8026 - recall: 0.4104 - val_auc: 0.9478 - val_loss: 5.9618e-04 - val_precision: 0.8605 - val_recall: 0.5362\n",
      "Epoch 33/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - auc: 0.9288 - loss: 6.4783e-04 - precision: 0.8479 - recall: 0.4809 - val_auc: 0.9471 - val_loss: 5.1777e-04 - val_precision: 0.8611 - val_recall: 0.4493\n",
      "Epoch 34/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - auc: 0.9170 - loss: 6.9113e-04 - precision: 0.8395 - recall: 0.4582 - val_auc: 0.9506 - val_loss: 5.6307e-04 - val_precision: 0.8409 - val_recall: 0.5362\n",
      "Epoch 35/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.9264 - loss: 5.1592e-04 - precision: 0.9036 - recall: 0.5010 - val_auc: 0.9554 - val_loss: 5.1444e-04 - val_precision: 0.9091 - val_recall: 0.4348\n",
      "Epoch 36/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.9492 - loss: 5.4061e-04 - precision: 0.8974 - recall: 0.5194 - val_auc: 0.9563 - val_loss: 5.2572e-04 - val_precision: 0.9143 - val_recall: 0.4638\n",
      "Epoch 37/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.9510 - loss: 6.5291e-04 - precision: 0.8411 - recall: 0.5179 - val_auc: 0.9537 - val_loss: 5.4217e-04 - val_precision: 0.8636 - val_recall: 0.5507\n",
      "Epoch 38/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.9591 - loss: 4.3354e-04 - precision: 0.8868 - recall: 0.5729 - val_auc: 0.9596 - val_loss: 5.1693e-04 - val_precision: 0.8696 - val_recall: 0.5797\n",
      "Epoch 39/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.9425 - loss: 4.9369e-04 - precision: 0.8526 - recall: 0.5736 - val_auc: 0.9636 - val_loss: 5.2170e-04 - val_precision: 0.8372 - val_recall: 0.5217\n",
      "Epoch 40/50\n",
      "\u001b[1m312/312\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc: 0.9435 - loss: 4.1797e-04 - precision: 0.9228 - recall: 0.5893 - val_auc: 0.9543 - val_loss: 5.1966e-04 - val_precision: 0.8444 - val_recall: 0.5507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model training completed and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# âœ… Define Early Stopping (to prevent overfitting)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# âœ… Train the Model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,  # Start with 50, can adjust based on performance\n",
    "    batch_size=512,  # Large batch sizes work well for fraud detection\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# âœ… Save the Model\n",
    "model.save(\"/Users/adityaiyer/Desktop/Credit-Card-Fraud-Detection/models/fraud_model.h5\")\n",
    "\n",
    "print(\"âœ… Model training completed and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d1440-9691-4132-abd7-6352532c358e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
